[{
    "avId": "MOD-000",
    "avName": "Adversarial Attacks",
    "info": [{
        "Description": "The attack taxonomy is based on the 2018 Pattern Recognition paper Wild Patterns: Ten years after the rise of adversarial machine learning, by Battista Biggio and Fabio Roli. The taxonomy is based on three layers, namely: (i) the attacker's capability, which is either test or training time and refers to what kind of data the attacker can manipulate; (ii) the attacker's goal, either integrity, availability or privacy/confidentiality; (iii) the attacker's knowledge, either full, partial, or zero knowledge, commonly known as, respectively, white, gray and black-box models.",
        "Impact": "Conduct a Software Supply Chain attack",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-100",
    "avName": "Test-Data",
    "info": [{
        "Description": "In Test-Data attacks, the attacker is capable to achieve its goal by manipulating only test data. Within this scenarios, the manipulation happens once the model has been deployed and is correctly operating. Depending on the goal of the attacker, we subdivide the attacks manipulating test data into attacks targeting integrity, availability or privacy/confidentiality.",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-110",
    "avName": "Test-Data - Integrity",
    "info": [{
        "Description": "These attacks aim to jeopardize the integrity of an ML models leveraging on test-data only. A typical attack falling under this category is evasion.",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-111",
    "avName": "Test-Data - Integrity - WhiteBox",
    "info": [{
        "Description": "When White-box, the attacker has full acces to the model and its parameters to create an attack.",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, 
{
    "avId": "MOD-112",
    "avName": "Test-Data - Integrity - GrayBox",
    "info": [{
        "Description": "When Gray-box, the attacker has only partial access to few details regarding the model",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-113",
    "avName": "Test-Data - Integrity - BlackBox",
    "info": [{
        "Description": "When Black-box, the attacker has no access to the model's parameters, but is typically allowed to query it.",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-120",
    "avName": "Test-Data - Availability",
    "info": [{
        "Description": "These attacks aim to jeopardize the availability of an ML models leveraging on test-data only. A typical attack falling under this category is sponge-attack.",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-121",
    "avName": "Test-Data - Availability - WhiteBox",
    "info": [{
        "Description": "When White-box, the attacker has full acces to the model and its parameters to create an attack.",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-122",
    "avName": "Test-Data - Availability - GrayBox",
    "info": [{
        "Description": "When Gray-box, the attacker has only partial access to few details regarding the model",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-123",
    "avName": "Test-Data - Availability - BlackBox",
    "info": [{
        "Description": "When Black-box, the attacker has no access to the model's parameters, but is typically allowed to query it.",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-130",
    "avName": "Test-Data - Privacy/Confidentiality",
    "info": [{
        "Description": "These attacks leverage on test-data to jeopardize the privacy/confidentiality of the model. A typical case is represented by Membership Inference attacks.",
        "Impact": "Execute Unauthorized Code",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-131",
    "avName": "Test-Data - Privacy/Confidentiality - WhiteBox",
    "info": [{
        "Description": "When White-box, the attacker has full acces to the model and its parameters to create an attack.",
        "Impact": "Execute Unauthorized Code",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-132",
    "avName": "Test-Data - Privacy/Confidentiality - GrayBox",
    "info": [{
        "Description": "When Gray-box, the attacker has only partial access to few details regarding the model",
        "Impact": "Acceptance of malicious code from package maintainer",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-133",
    "avName": "Test-Data - Privacy/Confidentiality - BlackBox",
    "info": [{
        "Description": "When Black-box, the attacker has no access to the model's parameters, but is typically allowed to query it.",
        "Impact": "Injection of malicious code into a legitimate software component",
        "Mapped Safeguard": []
    }]
},  {
    "avId": "MOD-200",
    "avName": "Training-Data",
    "info": [{
        "Description": "We refer to training data adversarial attacks, when the attackerâ€™s strategy is staged during the training phase. Some of the attacks staged during the training phase are then exploited at test-time (e.g., a backdoor, where the malicious pattern is triggered using specific test samples). The typical trainingdata attack, and the one to which we will focus for the rest of this section, is poisoning, which can jeopardize both the integrity and the availability of an ML model.",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-210",
    "avName": "Training-Data - Integrity",
    "info": [{
        "Description": "The typical attack targeting integrity based on training-data is represented by poisoning.",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-211",
    "avName": "Training-Data - Integrity - WhiteBox",
    "info": [{
        "Description": "When White-box, the attacker has full acces to the model and its parameters to create an attack.",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-212",
    "avName": "Training-Data - Integrity - GrayBox",
    "info": [{
        "Description": "When Gray-box, the attacker has only partial access to few details regarding the model",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-213",
    "avName": "Training-Data - Integrity - BlackBox",
    "info": [{
        "Description": "When Black-box, the attacker has no access to the model's parameters, but is typically allowed to query it.",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-220",
    "avName": "Training-Data - Availability",
    "info": [{
        "Description": "When targeting availability while leveraging on training-Data, we typically refer to attacks such as poisoning that, contrary to the standard version, aim to make the model be unable to normally classify (i.e., make them not available anymore).",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-221",
    "avName": "Training-Data - Availability - WhiteBox",
    "info": [{
        "Description": "When White-box, the attacker has full acces to the model and its parameters to create an attack.",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-222",
    "avName": "Training-Data - Availability - GrayBox",
    "info": [{
        "Description": "When Gray-box, the attacker has only partial access to few details regarding the model",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-223",
    "avName": "Training-Data - Availability - BlackBox",
    "info": [{
        "Description": "When Black-box, the attacker has no access to the model's parameters, but is typically allowed to query it.",
        "Impact": "Create name confusion, resulting in the installation of a malicious package",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-230",
    "avName": "Training-Data - Privacy/Confidentiality",
    "info": [{
        "Description": "The combination of training-data attacks jeopardizing privacy/confidentiality, remains yet to be explored. ",
        "Impact": "Execute Unauthorized Code",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-231",
    "avName": "Training-Data - Privacy/Confidentiality - WhiteBox",
    "info": [{
        "Description": "When White-box, the attacker has full acces to the model and its parameters to create an attack.",
        "Impact": "Execute Unauthorized Code",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-232",
    "avName": "Training-Data - Privacy/Confidentiality - GrayBox",
    "info": [{
        "Description": "When Gray-box, the attacker has only partial access to few details regarding the model",
        "Impact": "Acceptance of malicious code from package maintainer",
        "Mapped Safeguard": []
    }]
}, {
    "avId": "MOD-233",
    "avName": "Training-Data - Privacy/Confidentiality - BlackBox",
    "info": [{
        "Description": "When Black-box, the attacker has no access to the model's parameters, but is typically allowed to query it.",
        "Impact": "Injection of malicious code into a legitimate software component",
        "Mapped Safeguard": []
    }]
}]
